# Plan 007: FeedbackAgent Design - Learning from Generations and Evaluations

**Goal**: Design a FeedbackAgent that analyzes both what MutatorAgent GENERATES and what gets EVALUATED
**Context**: Current approach only looks at evaluated results. Need to see full picture: generations + evaluations
**Key Insight**: FeedbackAgent should see ALL generated variations, not just evaluated ones, to detect generation patterns and loops

---

## The Problem

### Current Limitation

**What FeedbackAgent currently sees:**
- Only evaluated prompts and their losses
- Which ones succeeded/failed

**What FeedbackAgent is missing:**
- What MutatorAgent GENERATED (all variations, before evaluation)
- Generation patterns (is mutator stuck in a loop?)
- Missing strategies (why isn't mutator trying role-play?)
- Diversity of generations (are all variations too similar?)

### Why This Matters

**Example scenario:**
```
Iteration 3:
  Generated: 
    1. "Analyze text sentiment. Reply with: positive, negative, or neutral."
    2. "Analyze text sentiment. Reply with: positive, negative, or neutral." (duplicate!)
    3. "Classify the sentiment as positive, negative, or neutral. Be concise."
  
  Evaluated:
    1. → 0.2667 (failed)
    2. → (duplicate, skipped)
    3. → 0.2667 (failed)
```

**What FeedbackAgent should detect:**
- ❌ MutatorAgent keeps generating "Reply with:" variations (even though they fail)
- ❌ MutatorAgent generated duplicate (waste of generation)
- ❌ All generations are similar (low diversity)
- ❌ MutatorAgent never tries role-play, examples, or other strategies

**What FeedbackAgent can guide:**
- "Stop generating 'Reply with:' variations, they keep failing"
- "Try generating role-play prompts, you haven't tried that"
- "Your generations lack diversity, try different structures"

---

## FeedbackAgent Design

### Core Responsibility

**Single focused task**: Analyze generation patterns and evaluation results to provide guidance for MutatorAgent.

**What it does:**
1. **Tracks generations**: Records all variations MutatorAgent generates
2. **Tracks evaluations**: Records which ones succeeded/failed
3. **Detects patterns**: Identifies what's being over-generated vs under-generated
4. **Provides guidance**: Tells MutatorAgent what to avoid and what to try

**What it doesn't do:**
- ❌ Generate prompts (that's MutatorAgent's job)
- ❌ Evaluate prompts (that's optimizer's job)
- ❌ Force diversity (that's DiversityAgent's job)
- ✅ Only analyzes and guides

---

## Architecture

### Data Flow

```
1. MutatorAgent generates variations
   ↓
2. FeedbackAgent.record_generations(variations)
   ↓
3. Optimizer evaluates variations
   ↓
4. FeedbackAgent.record_evaluations(results)
   ↓
5. FeedbackAgent.analyze() → detects patterns
   ↓
6. FeedbackAgent.get_guidance() → returns guidance
   ↓
7. Guidance passed to MutatorAgent in next iteration
```

### Class Design

```python
class FeedbackAgent:
    """Agent that learns from generation patterns and evaluation results."""
    
    def __init__(self):
        self.generation_history = []  # All generated prompts per iteration
        self.evaluation_history = []  # (prompt, loss, is_best) per iteration
        self.pattern_stats = {
            "successful_patterns": {},  # pattern -> success_count
            "failed_patterns": {},       # pattern -> failure_count
            "generation_frequency": {}   # pattern -> generation_count
        }
    
    def record_generations(
        self,
        iteration: int,
        variations: list[str],
        source_prompt: str
    ):
        """
        Record what MutatorAgent generated this iteration.
        
        Args:
            iteration: Current iteration number
            variations: All variations generated by MutatorAgent
            source_prompt: Prompt that was mutated from
        """
        self.generation_history.append({
            "iteration": iteration,
            "variations": variations,
            "source": source_prompt,
            "count": len(variations)
        })
        
        # Extract patterns from generations
        for variation in variations:
            patterns = self._extract_patterns(variation)
            for pattern, value in patterns.items():
                key = f"{pattern}:{value}"
                self.pattern_stats["generation_frequency"][key] = \
                    self.pattern_stats["generation_frequency"].get(key, 0) + 1
    
    def record_evaluations(
        self,
        iteration: int,
        results: list[tuple[str, float, bool]]  # (prompt, loss, is_best)
    ):
        """
        Record evaluation results for generated variations.
        
        Args:
            iteration: Current iteration number
            results: List of (prompt, loss, is_best) tuples
        """
        self.evaluation_history.append({
            "iteration": iteration,
            "results": results
        })
        
        # Extract patterns from evaluated prompts
        for prompt, loss, is_best in results:
            patterns = self._extract_patterns(prompt)
            for pattern, value in patterns.items():
                key = f"{pattern}:{value}"
                if is_best or loss < self._get_best_loss():
                    # Success
                    self.pattern_stats["successful_patterns"][key] = \
                        self.pattern_stats["successful_patterns"].get(key, 0) + 1
                else:
                    # Failure
                    self.pattern_stats["failed_patterns"][key] = \
                        self.pattern_stats["failed_patterns"].get(key, 0) + 1
    
    def analyze(self) -> dict:
        """
        Analyze generation and evaluation patterns.
        
        Returns:
            Dictionary with analysis results:
            - over_generated: Patterns generated too often but fail
            - under_generated: Strategies never tried
            - successful: Patterns that work
            - failed: Patterns that fail
            - diversity_issues: Low diversity in generations
        """
        analysis = {
            "over_generated": [],
            "under_generated": [],
            "successful": [],
            "failed": [],
            "diversity_issues": []
        }
        
        # Find over-generated patterns (generated often but fail)
        for pattern, gen_count in self.pattern_stats["generation_frequency"].items():
            fail_count = self.pattern_stats["failed_patterns"].get(pattern, 0)
            if gen_count >= 2 and fail_count >= gen_count * 0.7:  # 70%+ failure rate
                analysis["over_generated"].append({
                    "pattern": pattern,
                    "generated": gen_count,
                    "failed": fail_count
                })
        
        # Find missing strategies
        all_strategies = ["role_play", "examples", "chain_of_thought", "structured", "conversational"]
        tried_strategies = set()
        for pattern in self.pattern_stats["generation_frequency"].keys():
            if "is_role_play" in pattern:
                tried_strategies.add("role_play")
            elif "has_examples" in pattern:
                tried_strategies.add("examples")
            # ... check other strategies
        
        for strategy in all_strategies:
            if strategy not in tried_strategies:
                analysis["under_generated"].append(strategy)
        
        # Find successful patterns
        for pattern, count in sorted(
            self.pattern_stats["successful_patterns"].items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]:
            analysis["successful"].append({"pattern": pattern, "count": count})
        
        # Find failed patterns
        for pattern, count in sorted(
            self.pattern_stats["failed_patterns"].items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]:
            analysis["failed"].append({"pattern": pattern, "count": count})
        
        # Check diversity
        recent_generations = self.generation_history[-3:] if len(self.generation_history) >= 3 else self.generation_history
        diversity_score = self._calculate_diversity(recent_generations)
        if diversity_score < 0.3:  # Low diversity threshold
            analysis["diversity_issues"].append({
                "score": diversity_score,
                "message": "Recent generations are too similar"
            })
        
        return analysis
    
    def get_guidance(self) -> str:
        """
        Generate guidance for MutatorAgent based on analysis.
        
        Returns:
            String with actionable guidance
        """
        analysis = self.analyze()
        guidance_parts = []
        
        # Warn about over-generated patterns
        if analysis["over_generated"]:
            guidance_parts.append("⚠️  Stop generating these patterns (they keep failing):")
            for item in analysis["over_generated"][:3]:
                pattern_name = item["pattern"].split(":")[0]
                guidance_parts.append(f"   - {pattern_name} (generated {item['generated']}x, failed {item['failed']}x)")
        
        # Suggest missing strategies
        if analysis["under_generated"]:
            guidance_parts.append("\n✅ Try generating these strategies (never attempted):")
            for strategy in analysis["under_generated"][:3]:
                strategy_name = strategy.replace("_", " ").title()
                guidance_parts.append(f"   - {strategy_name} prompts")
        
        # Highlight successful patterns
        if analysis["successful"]:
            guidance_parts.append("\n✅ These patterns work well:")
            for item in analysis["successful"][:2]:
                pattern_name = item["pattern"].split(":")[0]
                guidance_parts.append(f"   - {pattern_name} (succeeded {item['count']}x)")
        
        # Warn about diversity
        if analysis["diversity_issues"]:
            guidance_parts.append("\n⚠️  Diversity issue:")
            guidance_parts.append(f"   - {analysis['diversity_issues'][0]['message']}")
            guidance_parts.append("   - Try generating prompts with different structures")
        
        return "\n".join(guidance_parts) if guidance_parts else "No specific guidance. Continue exploring."
    
    def _extract_patterns(self, prompt: str) -> dict:
        """Extract patterns from a prompt."""
        prompt_lower = prompt.lower()
        return {
            "has_reply_with": "reply with" in prompt_lower,
            "has_be_concise": "be concise" in prompt_lower or "be brief" in prompt_lower,
            "has_one_word": "one word" in prompt_lower or "single word" in prompt_lower,
            "has_format_spec": any(word in prompt_lower for word in ["format:", "output:", "respond with:"]),
            "is_imperative": prompt[0].isupper() and not prompt.lower().startswith("you"),
            "is_role_play": prompt.lower().startswith("you are"),
            "has_examples": "example" in prompt_lower or "e.g." in prompt_lower,
            "is_chain_of_thought": any(phrase in prompt_lower for phrase in ["first,", "step", "think"]),
            "is_structured": "[" in prompt or ":" in prompt and "\n" in prompt,
            "is_conversational": prompt.lower().startswith(("what", "how", "tell me", "can you")),
            "length": len(prompt.split()),
            "starts_with": prompt.split()[0].lower() if prompt.split() else ""
        }
    
    def _calculate_diversity(self, generations: list[dict]) -> float:
        """Calculate diversity score of recent generations."""
        if not generations:
            return 1.0
        
        all_variations = []
        for gen in generations:
            all_variations.extend(gen["variations"])
        
        if len(all_variations) < 2:
            return 1.0
        
        # Simple diversity: unique prompts / total prompts
        unique = set(all_variations)
        return len(unique) / len(all_variations)
    
    def _get_best_loss(self) -> float:
        """Get best loss from evaluation history."""
        best = float('inf')
        for eval_data in self.evaluation_history:
            for _, loss, is_best in eval_data["results"]:
                if is_best:
                    best = min(best, loss)
        return best if best != float('inf') else 1.0
```

---

## Integration with Optimizer

### Step 1: Add FeedbackAgent to Optimizer

```python
# In optimizer.py
class PromptOptimizer:
    def __init__(
        self,
        ...
        feedback_agent: Optional['FeedbackAgent'] = None,
    ):
        ...
        self.feedback_agent = feedback_agent
```

### Step 2: Record Generations

```python
# In optimizer.py, in optimize() loop
for iteration in range(1, self.max_iterations + 1):
    ...
    # Generate variations
    variations = self.mutator.mutate(current_prompt, context)
    
    # Record what was generated
    if self.feedback_agent:
        self.feedback_agent.record_generations(
            iteration=iteration,
            variations=variations,
            source_prompt=current_prompt
        )
    
    # Evaluate variations
    variation_losses = []
    for variant in variations:
        variant_loss = self._evaluate_prompt(variant, test_cases)
        variation_losses.append((variant, variant_loss))
        ...
    
    # Record evaluation results
    if self.feedback_agent:
        results = [
            (variant, loss, loss < best_loss)
            for variant, loss in variation_losses
        ]
        self.feedback_agent.record_evaluations(iteration, results)
    
    # Get guidance for next iteration
    if self.feedback_agent:
        guidance = self.feedback_agent.get_guidance()
        # Store for next iteration
        self.mutation_guidance = guidance
```

### Step 3: Pass Guidance to Mutator

```python
# In next iteration, pass guidance in context
context = {
    "iteration": iteration,
    "current_loss": current_loss,
    "best_loss": best_loss,
    "guidance": self.mutation_guidance  # ← FeedbackAgent's guidance
}
variations = self.mutator.mutate(current_prompt, context)
```

### Step 4: MutatorAgent Uses Guidance

```python
# In MutatorAgent._build_instruction()
def _build_instruction(self, prompt: str, context: dict) -> str:
    ...
    guidance = context.get("guidance", "")
    
    instruction = MUTATOR_INSTRUCTION_TEMPLATE.format(
        num_variations=self.num_variations,
        prompt=prompt,
        loss_info=loss_info,
        guidance=guidance  # ← Add guidance to instruction
    )
    return instruction
```

### Step 5: Update Mutator Prompt Template

```python
# In constants/prompts.py
MUTATOR_INSTRUCTION_TEMPLATE = """...existing template...

{guidance}

Generate {num_variations} variations..."""
```

---

## Concrete Examples

### Example 1: Detecting Generation Loop

**Iteration 3:**
```
Generated:
  1. "Analyze text sentiment. Reply with: positive, negative, or neutral."
  2. "Analyze text sentiment. Reply with: positive, negative, or neutral." (duplicate)
  3. "Classify the sentiment as positive, negative, or neutral. Be concise."

Evaluated:
  1. → 0.2667 (failed)
  2. → (duplicate, skipped)
  3. → 0.2667 (failed)
```

**FeedbackAgent analysis:**
```python
{
    "over_generated": [
        {"pattern": "has_reply_with:True", "generated": 2, "failed": 1},
        {"pattern": "has_be_concise:True", "generated": 1, "failed": 1}
    ],
    "under_generated": ["role_play", "examples", "chain_of_thought"],
    "diversity_issues": [{"score": 0.33, "message": "Recent generations are too similar"}]
}
```

**FeedbackAgent guidance:**
```
⚠️  Stop generating these patterns (they keep failing):
   - has_reply_with (generated 2x, failed 1x)
   - has_be_concise (generated 1x, failed 1x)

✅ Try generating these strategies (never attempted):
   - Role Play prompts
   - Examples prompts
   - Chain Of Thought prompts

⚠️  Diversity issue:
   - Recent generations are too similar
   - Try generating prompts with different structures
```

**Result:** MutatorAgent avoids "Reply with:" and tries role-play in next iteration.

---

### Example 2: Learning What Works

**Iteration 1:**
```
Generated:
  1. "Determine sentiment: positive, negative, or neutral. One word only."
  2. "Classify the sentiment as positive, negative, or neutral. Be concise."
  3. "Analyze text sentiment. Reply with: positive, negative, or neutral."

Evaluated:
  1. → 0.2333 (best!) ✓
  2. → 0.2667 (failed)
  3. → 0.2667 (failed)
```

**FeedbackAgent analysis:**
```python
{
    "successful": [
        {"pattern": "has_one_word:True", "count": 1},
        {"pattern": "has_format_spec:True", "count": 1},
        {"pattern": "starts_with:determine", "count": 1}
    ],
    "failed": [
        {"pattern": "has_be_concise:True", "count": 1},
        {"pattern": "has_reply_with:True", "count": 1}
    ]
}
```

**FeedbackAgent guidance:**
```
✅ These patterns work well:
   - has_one_word (succeeded 1x)
   - has_format_spec (succeeded 1x)

⚠️  Stop generating these patterns (they keep failing):
   - has_be_concise (generated 1x, failed 1x)
   - has_reply_with (generated 1x, failed 1x)
```

**Result:** MutatorAgent generates more "One word" variations in next iteration.

---

### Example 3: Diversity Detection

**Iterations 2-4:**
```
All generations are variations of:
  - "Classify sentiment..."
  - "Determine sentiment..."
  - "Analyze sentiment..."

No role-play, examples, or other strategies.
```

**FeedbackAgent analysis:**
```python
{
    "diversity_issues": [
        {"score": 0.25, "message": "Recent generations are too similar"}
    ],
    "under_generated": ["role_play", "examples", "chain_of_thought", "structured"]
}
```

**FeedbackAgent guidance:**
```
⚠️  Diversity issue:
   - Recent generations are too similar
   - Try generating prompts with different structures

✅ Try generating these strategies (never attempted):
   - Role Play prompts
   - Examples prompts
   - Chain Of Thought prompts
```

**Result:** MutatorAgent tries different structures, improving exploration.

---

## Benefits

### 1. Prevents Generation Loops
- Detects when MutatorAgent keeps generating failing patterns
- Stops waste of evaluations on known-bad patterns

### 2. Identifies Missing Strategies
- Detects strategies never tried (role-play, examples, etc.)
- Guides MutatorAgent to explore new directions

### 3. Learns from Success
- Identifies what works
- Reinforces successful patterns

### 4. Measures Diversity
- Detects when generations are too similar
- Encourages more diverse exploration

### 5. Actionable Guidance
- Provides specific, actionable feedback
- Not just analysis, but direction

---

## Testing Plan

### Test 1: Pattern Detection
```python
feedback_agent = FeedbackAgent()

# Record generations
feedback_agent.record_generations(1, [
    "Reply with: positive, negative, or neutral.",
    "Be concise. Reply with: positive, negative, or neutral."
], "Classify sentiment")

# Record evaluations
feedback_agent.record_evaluations(1, [
    ("Reply with: positive, negative, or neutral.", 0.3, False),
    ("Be concise. Reply with: positive, negative, or neutral.", 0.3, False)
])

# Check analysis
analysis = feedback_agent.analyze()
assert "has_reply_with" in str(analysis["over_generated"])
```

### Test 2: Missing Strategies
```python
# Record only imperative prompts
feedback_agent.record_generations(1, [
    "Classify sentiment",
    "Determine sentiment"
], "Classify sentiment")

analysis = feedback_agent.analyze()
assert "role_play" in analysis["under_generated"]
```

### Test 3: Guidance Generation
```python
guidance = feedback_agent.get_guidance()
assert "Stop generating" in guidance or "Try generating" in guidance
```

---

## Implementation Priority

### Priority: MEDIUM

**Why not P1 (highest)?**
- Quick fixes (deduplication, patience) are faster
- DiversityAgent solves core diversity issue
- FeedbackAgent is enhancement, not critical

**Why not P3 (lowest)?**
- Provides real value (prevents waste)
- Relatively simple to implement
- Complements other agents well

**Recommended order:**
1. Phase 1: Quick fixes (20 min)
2. Phase 2: DiversityAgent (1-2 hours)
3. Phase 3: FeedbackAgent (2-3 hours) ← This plan
4. Phase 4: ExplorationAgent (if needed)

---

## Integration Checklist

- [ ] Create `agents/feedback_agent.py`
- [ ] Add `FeedbackAgent` class with pattern extraction
- [ ] Add `record_generations()` method
- [ ] Add `record_evaluations()` method
- [ ] Add `analyze()` method
- [ ] Add `get_guidance()` method
- [ ] Integrate into `optimizer.py`:
  - [ ] Add `feedback_agent` parameter
  - [ ] Call `record_generations()` after mutation
  - [ ] Call `record_evaluations()` after evaluation
  - [ ] Pass guidance in context to mutator
- [ ] Update `MutatorAgent` to use guidance
- [ ] Update `MUTATOR_INSTRUCTION_TEMPLATE` to include guidance
- [ ] Add tests for pattern detection
- [ ] Test on challenging dataset

---

## Conclusion

**FeedbackAgent** provides focused value:
- ✅ Sees full picture (generations + evaluations)
- ✅ Detects generation loops and missing strategies
- ✅ Provides actionable guidance
- ✅ Simple, focused responsibility
- ✅ Complements other agents

**Key insight**: By seeing both what's generated and what's evaluated, FeedbackAgent can detect problems earlier and provide better guidance than just looking at evaluations alone.

